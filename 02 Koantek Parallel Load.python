{"version":"NotebookV1","origId":1343992488731494,"name":"02 Koantek Parallel Load","language":"python","commands":[{"version":"CommandV1","origId":1343992488731495,"guid":"a6933bc7-e4d0-4b7c-bc80-f91cc28e4cc6","subtype":"command","commandType":"auto","position":0.125,"command":"dbutils.widgets.text(\"configuration_file\",\"dbfs:/LoadConfig/sample.json\")","commandVersion":21,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{"configuration_file":{"widgetType":"text","name":"configuration_file","defaultValue":"dbfs:/LoadConfig/sample.json","label":null,"options":{"widgetType":"text","validationRegex":null}}},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1690208780010,"submitTime":1690208779986,"finishTime":1690208780091,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"72e14cbe-5a84-472f-8839-d1086e94b191"},{"version":"CommandV1","origId":1343992488731496,"guid":"b648943e-2821-459a-b626-1340f1d586ef","subtype":"command","commandType":"auto","position":0.203125,"command":"from datetime import datetime, time\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import *\nimport time\nimport json\nimport ast\n\ndef koantekParallelLoad():\n    json_cfg = dbutils.widgets.get(\"configuration_file\")\n    with open(json_cfg, 'r') as openfile:\n        json_cfg = ast.literal_eval(openfile.read())        \n    dbcfg = json_cfg[\"config\"]\n    jdbcHostname = dbcfg[\"jdbcHostname\"]\n    jdbcPort = dbcfg[\"jdbcPort\"]\n    jdbcDatabase = dbcfg[\"jdbcSourceDatabase\"]\n    jdbcUsername = dbcfg[\"jdbcUsername\"]\n    jdbcPassword = dbcfg[\"jdbcPassword\"]\n    jdbcDriver = dbcfg[\"jdbcDriver\"]\n    jdbcSchema = dbcfg[\"jdbcSourceSchema\"]\n    #targetDatabase = dbcfg[\"targetDatabase\"]\n    targetSchema = dbcfg[\"targetSchema\"]\n    numPartitions = dbcfg[\"parallelCores\"]\n    jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};    user={jdbcUsername};password={jdbcPassword};TrustServerCertificate=True\" # ;    currentSchema={jdbcSchema}\n    #print (jdbcUrl)\n    for k in json_cfg[\"load\"]:\n        tbl = k[\"table\"]\n        keyField = k[\"keyField\"]\n        partition = k.get(\"partition\") if k.get(\"partition\") else None\n        z_order = \",\".join( k.get(\"zorder\")) if k.get(\"zorder\") else None\n        cluster = \",\".join( k.get(\"cluster\")) if k.get(\"cluster\") else None\n        print(f\"{jdbcSchema}.{tbl}\")\n        tblconfig = (\n            spark.read.jdbc(url=jdbcUrl, table=f\"{jdbcSchema}.{tbl}\")\n                .select(\n                    F.min(F.col(keyField)).alias(\"min_value\"),\n                    F.max(col(keyField)).alias(\"max_value\"),\n                    F.count(\"*\").alias(\"row_count\"))  )  \n        # Extract the values from the result\n        lower = int(tblconfig.collect()[0][\"min_value\"])\n        #print(f\"min_value {lower}\")\n        upper = int(tblconfig.collect()[0][\"max_value\"])\n        #print(f\"max_value {upper}\")\n        row_count = int(tblconfig.collect()[0][\"row_count\"])\n        #print(f\"row_count {row_count}\")\n        print(str(datetime.now()) + \"> Starting to import \" + tbl)\n        dft = spark.read.jdbc(url=jdbcUrl, table=f\"{jdbcSchema}.{tbl}\"\n                            , column=keyField, lowerBound=lower, upperBound=upper, numPartitions=numPartitions)\n        tries = 3\n\n        for i in range(tries):\n            try:\n                start_time =    str(datetime.now())\n                #spark.catalog.setCurrentDatabase(targetDatabase)\n                if(partition and cluster == None):\n                    print(str(start_time) + f\"> Partitioning {targetSchema}.{tbl} by {partition}\")\n                    dft.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(partition).saveAsTable(f\"{targetSchema}.{tbl}\")\n                else:\n                    dft.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{targetSchema}.{tbl}\")\n            except Exception as e:\n                if i < tries - 1: \n                    print(str(datetime.now()) + f\"> Exception attempting to sync for {targetSchema}.{tbl} - retrying in 15 seconds. Exception: {e}\")\n                    time.sleep(15)\n                    continue\n                else:\n                    print(str(datetime.now()) + f\"> MAX number of retries reached, Exception: {e}\")\n                raise\n            break\n        if(cluster):\n            print(str(datetime.now()) + f\"> Clustering {targetSchema}.{tbl} by {cluster}\")\n            spark.sql(f\"CREATE TABLE {targetSchema}.{tbl}_temp CLUSTER BY ({cluster}) AS SELECT * FROM {targetSchema}.{tbl}\")\n            spark.sql(f\"DROP TABLE {targetSchema}.{tbl}\")\n            spark.sql(f\"ALTER TABLE {targetSchema}.{tbl}_temp RENAME TO {targetSchema}.{tbl}\")            \n        if(z_order and cluster == None):\n            print(str(datetime.now()) + f\"> Optimizing {targetSchema}.{tbl} by {z_order}\")\n            spark.sql(f\"OPTIMIZE {targetSchema}.{tbl} ZORDER BY ({z_order})\")\n        end_time = str(datetime.now())\n        end_rowcount = spark.read.table(f\"{targetSchema}.{tbl}\").select(\n            F.count(\"*\").alias(\"row_count\")).collect()[0].asDict()[\"row_count\"]\n     \n        print(end_time + f\"> Import completed for {targetSchema}.{tbl}\")\n        new_row = Row(\n        f\"{targetSchema}.{tbl}\" # table_name\tSTRINg\n        , start_time# ,load_begin\tTIMESTAMP\n        , end_time # ,load_end\tTIMESTAMP\n        , row_count # ,begin_rowcount INT\n        , end_rowcount # ,end_rowcount INT\n        , end_rowcount/(\n            (datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S.%f\") - datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S.%f\")).total_seconds()\n            )# ,rows_per_minute FLOAT\n        # Insert the row into the table\n        )\n\n        # Convert the row to a DataFrame\n        new_row_df = spark.createDataFrame([new_row])\n        new_row_df.write.insertInto(f\"{targetSchema}.load_metrics\", overwrite=False)","commandVersion":136,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1690213116898,"submitTime":1690213116871,"finishTime":1690213116986,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"Definition of Parallel Load Function","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"125a3493-2ea7-425e-8a72-7fc207c5d278"},{"version":"CommandV1","origId":1343992488731497,"guid":"7ef94cbd-e642-4d74-906f-a9e650d86352","subtype":"command","commandType":"auto","position":0.2265625,"command":"koantekParallelLoad()","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"triyam.order_entry_fields\n2023-07-24 15:38:43.889864> Starting to import order_entry_fields\n2023-07-24 15:38:48.577270> Import completed for koantek_triyam_test.order_entry_fields\ntriyam.oe_format_fields\n2023-07-24 15:38:54.031378> Starting to import oe_format_fields\n2023-07-24 15:38:58.166427> Clustering koantek_triyam_test.oe_format_fields by EPILOG_METHOD\n2023-07-24 15:39:04.836987> Import completed for koantek_triyam_test.oe_format_fields\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{"configuration_file":"/dbfs/LoadConfig/sample.json"},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1690213123483,"submitTime":1690213123455,"finishTime":1690213149956,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"Execution of Parallel Load","showCommandTitle":true,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",447]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"ebc3e10f-b666-44dc-a74a-d9e9e22509df"}],"dashboards":[],"guid":"728c6941-512d-435c-9779-d1b78448ed34","globalVars":{},"iPythonMetadata":null,"inputWidgets":{"configuration_file":{"nuid":"5450a7e0-3f51-446e-a21e-bf7ff7eccba8","currentValue":"/dbfs/LoadConfig/sample.json","widgetInfo":{"widgetType":"text","name":"configuration_file","defaultValue":"dbfs:/LoadConfig/sample.json","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1343992488731351,"dataframes":["_sqldf"]}},"reposExportFormat":"SOURCE"}