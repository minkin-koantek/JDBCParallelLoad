{"version":"NotebookV1","origId":1343992488731471,"name":"03 Execution runs from NCH load prior to import JSON support","language":"python","commands":[{"version":"CommandV1","origId":1343992488731472,"guid":"9cfab067-7cd7-4986-85bd-24ce3d26cad5","subtype":"command","commandType":"auto","position":0.25,"command":"%md\n# Koantek Parallel Load Utility\n\n","commandVersion":105,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":null,"subcommandOptions":null,"contentSha256Hex":null,"nuid":"e998cf45-c1ed-4066-9e8f-68965b463044"},{"version":"CommandV1","origId":1343992488731479,"guid":"b5c5c24a-ac25-4cf8-80e1-694a0027d5ff","subtype":"command","commandType":"auto","position":1.00390625,"command":"newcfg = {\n    \"config\":\n        {\n            # ALL FIELDS ARE MANDATORY IN THIS DICTIONARY\n             \"jdbcHostname\":\"10.10.9.4\" # Source Server Address\n            ,\"jdbcPort\":1433    # Source Server Port\n            ,\"jdbcSourceDatabase\":\"NCH_Triyam\" # Source Server Database \n            ,\"jdbcUsername\":\"koantekUser\" # Source Server user\n            ,\"jdbcPassword\":\"aBjO40oTOy&4l0\" # Source Server password\n            ,\"jdbcDriver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\" # Source Server JDBC Driver\n            ,\"jdbcSourceSchema\":\"triyam\" # Source Database Schema\n            #,\"targetDatabase\":\"koantek_parallel_load\" Target databse for Unity Catalog implementation\n            ,\"targetSchema\":\"koantek_triyam_test\" # Target Server Schema\n            ,\"parallelCores\":16 # Number of vCPUs available in cluster\n        }\n        ,\"load\":\n[     \n    {\n        \"table\":\"ACCESSION_ORDER_R\"\n        ,\"keyField\": \"ORDER_ID\"}\n    ,{\n        \"table\":\"ACT_PW_COMP_1\"\n        ,\"keyField\": \"ACT_PW_COMP_ID\"}\n    ,{\n        \"table\":\"ADDRESS\"\n        ,\"keyField\": \"ADDRESS_ID\"}\n    ,{\n        \"table\":\"CE_CALCULATION_RESULT\"\n        ,\"keyField\": \"EVENT_ID\"}\n    ,{\n        \"table\":\"CE_CONTRIBUTOR_LINK\"\n        ,\"keyField\":\"CONTRIBUTOR_EVENT_ID\"}\n    ,{\n        \"table\":\"CE_DATE_RESULT\"\n        ,\"keyField\": \"EVENT_ID\"}\n    ,{\n        \"table\":\"CE_EVENT_NOTE\"\n        ,\"keyField\": \"CE_EVENT_NOTE_ID\"}\n    ,{\n        \"table\":\"ce_event_order_link\"\n        ,\"keyField\":\"EVENT_ID\"}\n    ,\n    {\n        \"table\":\"CE_EVENT_PRSNL\"\n        ,\"keyField\":\"CE_EVENT_PRSNL_ID\"}\n    ,{\n        \"table\":\"CE_INTAKE_OUTPUT_RESULT\"\n        ,\"keyField\": \"CE_IO_RESULT_ID\"}\n    ,{\n        \"table\":\"CE_MED_RESULT\"\n        ,\"keyField\": \"EVENT_ID\"}\n    ,{\n        \"table\":\"CE_RESULT_SET_LINK\"\n        ,\"keyField\": \"EVENT_ID\"}\n    ,{\n        \"table\":\"CE_SPECIMEN_COLL\"\n        ,\"keyField\": \"EVENT_ID\"}\n    ,{\n        \"table\":\"clinical_event\"\n        ,\"keyField\":\"CLINICAL_EVENT_ID\"}\n    ,{\n        \"table\":\"CODE_VALUE\"\n        ,\"keyField\": \"CODE_VALUE\"}\n    ,{\n        \"table\":\"CUSTOM_ES_HIER_DISCRETE\"\n        ,\"keyField\": \"EVENT_CD\"}\n    ,{\n        \"table\":\"CUSTOM_RES_COMMENT_DEST\"\n        ,\"keyField\": \"PARENT_ENTITY_ID\"}\n    ,{\n        \"table\":\"DCP_FORMS_ACTIVITY_COMP\"\n        ,\"keyField\": \"DCP_FORMS_ACTIVITY_COMP_ID\"}\n    ,{\n        \"table\":\"DCP_FORMS_ACTIVITY\"\n        ,\"keyField\": \"DCP_FORMS_ACTIVITY_ID\"}\n    ,{\n        \"table\":\"LONG_TEXT\"\n        ,\"keyField\": \"LONG_TEXT_ID\"}\n    ,{\n        \"table\":\"MESSAGING_AUDIT\"\n        ,\"keyField\": \"MESSAGING_AUDIT_ID\"}\n    ,{\n        \"table\":\"MIC_TASK_LOG\"\n        ,\"keyField\":\"TASK_LOG_ID\"}\n    ,{\n        \"table\":\"NOMENCLATURE\"\n        ,\"keyField\":\"NOMENCLATURE_ID\"}\n    ,{\n        \"table\":\"oe_format_fields\"\n        ,\"keyField\":\"OE_FORMAT_ID\"}\n    ,{\n        \"table\":\"ORDER_ACTION\"\n        ,\"keyField\": \"ORDER_ID\"}\n    ,{\n        \"table\":\"ORDER_CATALOG_SYNONYM\"\n        ,\"keyField\": \"CATALOG_CD\"}\n    ,{\n        \"table\":\"ORDER_CATALOG\"\n        ,\"keyField\": \"CATALOG_CD\"}\n    ,{\n        \"table\":\"ORDER_COMMENT\"\n        ,\"keyField\": \"ORDER_ID\"}\n    ,{\n        \"table\":\"order_detail\"\n        ,\"keyField\":\"ORDER_ID\"}\n    ,{\n        \"table\":\"order_entry_fields\"\n        ,\"keyField\":\"OE_FIELD_ID\"}\n    ,{\n        \"table\":\"ORDER_INGREDIENT\"\n        ,\"keyField\":\"ORDER_ID\"}\n    ,{\n        \"table\":\"ORDER_PRODUCT\"\n        ,\"keyField\":\"ORDER_ID\"}\n    ,{\n        \"table\":\"ORDER_REVIEW\"\n        ,\"keyField\":\"ORDER_ID\"}    \n    ,{\n        \"table\":\"ORDERS\"\n        ,\"keyField\": \"ORDER_ID\"}\n    ,{\n        \"table\":\"ORGANIZATION\"\n        ,\"keyField\": \"ORGANIZATION_ID\"}\n    ,{\n        \"table\":\"PATHWAY\"\n        ,\"keyField\": \"PATHWAY_ID\"}\n    ,{\n        \"table\":\"qa100_ce_event_order_link\"\n        ,\"keyField\":\"EVENT_ID\"}\n    ,{\n        \"table\":\"qa100_CE_EVENT_PRSNL\"\n        ,\"keyField\":\"CE_EVENT_PRSNL_ID\"}\n    ,{\n        \"table\":\"qa100_clinical_event\"\n        ,\"keyField\":\"CLINICAL_EVENT_ID\"}\n    ,{\n        \"table\":\"QA100_ORDERS\"\n        ,\"keyField\":\"ORDER_ID\"}\n    ,{\n        \"table\":\"Register_fovea_5252022_final\"\n        ,\"keyField\": \"CERNER_PERSON_ID\"}\n    ,{\n        \"table\":\"Tri_Encounter\"\n        ,\"keyField\": \"ENCNTR_ID\"}\n    ,{\n        \"table\":\"TRI_Person\"\n        ,\"keyField\": \"PERSON_ID\"}\n    ,{\n        \"table\":\"Tri_PRSNL\"\n        ,\"keyField\": \"PERSON_ID\"}\n]    \n}","commandVersion":14,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1685558327134,"submitTime":1685558327096,"finishTime":1685558327191,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"799f2ca1-b719-4925-b176-90c54b752fa1"},{"version":"CommandV1","origId":1343992488731480,"guid":"66fdb9d3-e365-46a2-a5b5-c4ffaab610c0","subtype":"command","commandType":"auto","position":1.009765625,"command":"koantekParallelLoad(newcfg)","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"2023-05-31 18:39:17.152782> Starting to import ACCESSION_ORDER_R\n2023-05-31 18:41:25.083389> Import completed for koantek_triyam_test.ACCESSION_ORDER_R\n2023-05-31 18:41:55.401750> Starting to import ACT_PW_COMP_1\n2023-05-31 18:48:07.913998> Import completed for koantek_triyam_test.ACT_PW_COMP_1\n2023-05-31 18:48:32.363474> Starting to import ADDRESS\n2023-05-31 18:51:39.135285> Import completed for koantek_triyam_test.ADDRESS\n2023-05-31 18:51:58.984624> Starting to import CE_CALCULATION_RESULT\n2023-05-31 18:53:12.866192> Import completed for koantek_triyam_test.CE_CALCULATION_RESULT\n2023-05-31 18:53:36.585902> Starting to import CE_CONTRIBUTOR_LINK\n2023-05-31 18:55:12.275736> Import completed for koantek_triyam_test.CE_CONTRIBUTOR_LINK\n2023-05-31 18:55:31.508891> Starting to import CE_DATE_RESULT\n2023-05-31 18:57:33.235019> Import completed for koantek_triyam_test.CE_DATE_RESULT\n2023-05-31 18:57:57.581362> Starting to import CE_EVENT_NOTE\n2023-05-31 19:00:51.855271> Import completed for koantek_triyam_test.CE_EVENT_NOTE\n2023-05-31 19:06:28.955851> Starting to import ce_event_order_link\n2023-05-31 19:40:08.629962> Import completed for koantek_triyam_test.ce_event_order_link\n2023-05-31 19:59:23.393229> Starting to import CE_EVENT_PRSNL\n2023-05-31 23:01:49.607912> Import completed for koantek_triyam_test.CE_EVENT_PRSNL\n2023-05-31 23:02:03.082849> Starting to import CE_INTAKE_OUTPUT_RESULT\n2023-05-31 23:05:27.096515> Import completed for koantek_triyam_test.CE_INTAKE_OUTPUT_RESULT\n2023-05-31 23:05:48.489363> Starting to import CE_MED_RESULT\n2023-05-31 23:12:16.547928> Import completed for koantek_triyam_test.CE_MED_RESULT\n2023-05-31 23:13:14.528628> Starting to import CE_RESULT_SET_LINK\n2023-05-31 23:19:24.167893> Import completed for koantek_triyam_test.CE_RESULT_SET_LINK\n2023-05-31 23:19:56.093858> Starting to import CE_SPECIMEN_COLL\n2023-05-31 23:24:20.664260> Import completed for koantek_triyam_test.CE_SPECIMEN_COLL\n2023-05-31 23:39:40.387009> Starting to import clinical_event\n2023-06-01 00:31:46.179112> Exception attempting to sync for koantek_triyam_test.clinical_event - retrying in 15 seconds. Exception: An error occurred while calling o1304.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 386.0 failed 4 times, most recent failure: Lost task 9.3 in stage 386.0 (TID 944) (10.139.64.7 executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Command exited with code 50\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3481)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3413)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1449)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3701)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3639)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3627)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1184)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2753)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2736)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$1(FileFormatWriter.scala:299)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:330)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:251)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:71)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$3(commands.scala:132)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:130)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:129)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$doExecute$4(commands.scala:156)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:156)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:273)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:107)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:330)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:326)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:269)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:358)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:357)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$11(TransactionalWriteEdge.scala:546)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:541)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:654)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:675)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:459)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:444)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:649)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:568)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:529)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:279)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1981)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:278)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:311)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:305)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:592)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:582)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:375)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:172)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:654)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:675)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:459)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:444)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:649)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:568)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:529)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:264)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:93)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:126)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:913)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:93)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:872)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:673)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1751)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:654)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:683)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:649)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:229)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:288)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:287)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:725)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n2023-06-01 06:41:49.852494> Import completed for koantek_triyam_test.clinical_event\n2023-06-01 06:42:01.023792> Starting to import CODE_VALUE\n2023-06-01 06:42:11.891083> Import completed for koantek_triyam_test.CODE_VALUE\n2023-06-01 06:42:14.840047> Starting to import CUSTOM_ES_HIER_DISCRETE\n2023-06-01 06:42:18.264397> Import completed for koantek_triyam_test.CUSTOM_ES_HIER_DISCRETE\n2023-06-01 06:42:35.657349> Starting to import CUSTOM_RES_COMMENT_DEST\n2023-06-01 06:46:18.141947> Import completed for koantek_triyam_test.CUSTOM_RES_COMMENT_DEST\n2023-06-01 06:46:59.962137> Starting to import DCP_FORMS_ACTIVITY_COMP\n2023-06-01 06:51:13.176581> Import completed for koantek_triyam_test.DCP_FORMS_ACTIVITY_COMP\n2023-06-01 06:51:36.795469> Starting to import DCP_FORMS_ACTIVITY\n2023-06-01 06:57:02.468369> Import completed for koantek_triyam_test.DCP_FORMS_ACTIVITY\n2023-06-01 06:58:42.310575> Starting to import LONG_TEXT\n2023-06-01 07:16:09.455071> Import completed for koantek_triyam_test.LONG_TEXT\n2023-06-01 07:16:27.171651> Starting to import MESSAGING_AUDIT\n2023-06-01 07:18:25.997933> Import completed for koantek_triyam_test.MESSAGING_AUDIT\n2023-06-01 07:18:33.332745> Starting to import MIC_TASK_LOG\n2023-06-01 07:18:51.697740> Import completed for koantek_triyam_test.MIC_TASK_LOG\n2023-06-01 07:18:59.684863> Starting to import NOMENCLATURE\n2023-06-01 07:19:20.735253> Import completed for koantek_triyam_test.NOMENCLATURE\n2023-06-01 07:19:23.561841> Starting to import oe_format_fields\n2023-06-01 07:19:27.908803> Import completed for koantek_triyam_test.oe_format_fields\n2023-06-01 07:22:40.256506> Starting to import ORDER_ACTION\n2023-06-01 08:29:26.560589> Import completed for koantek_triyam_test.ORDER_ACTION\n2023-06-01 08:29:31.051989> Starting to import ORDER_CATALOG_SYNONYM\n2023-06-01 08:29:37.222568> Import completed for koantek_triyam_test.ORDER_CATALOG_SYNONYM\n2023-06-01 08:29:40.193689> Starting to import ORDER_CATALOG\n2023-06-01 08:29:44.075883> Import completed for koantek_triyam_test.ORDER_CATALOG\n2023-06-01 08:30:36.247095> Starting to import ORDER_COMMENT\n2023-06-01 08:36:55.075420> Import completed for koantek_triyam_test.ORDER_COMMENT\n2023-06-01 08:56:32.053083> Starting to import order_detail\n2023-06-01 10:31:13.741660> Import completed for koantek_triyam_test.order_detail\n2023-06-01 10:31:17.909108> Starting to import order_entry_fields\n2023-06-01 10:31:21.600945> Import completed for koantek_triyam_test.order_entry_fields\n2023-06-01 10:31:36.869265> Starting to import ORDER_INGREDIENT\n2023-06-01 10:36:47.835752> Import completed for koantek_triyam_test.ORDER_INGREDIENT\n2023-06-01 10:37:01.548244> Starting to import ORDER_PRODUCT\n2023-06-01 10:38:38.500079> Import completed for koantek_triyam_test.ORDER_PRODUCT\n2023-06-01 10:39:32.708812> Starting to import ORDER_REVIEW\n2023-06-01 10:47:02.275444> Import completed for koantek_triyam_test.ORDER_REVIEW\n2023-06-01 10:48:16.764664> Starting to import ORDERS\n2023-06-01 11:44:53.616447> Import completed for koantek_triyam_test.ORDERS\n2023-06-01 11:45:02.565040> Starting to import ORGANIZATION\n2023-06-01 11:45:10.004125> Import completed for koantek_triyam_test.ORGANIZATION\n2023-06-01 11:45:14.081169> Starting to import PATHWAY\n2023-06-01 11:46:38.403859> Import completed for koantek_triyam_test.PATHWAY\n2023-06-01 11:46:42.918672> Starting to import qa100_ce_event_order_link\n2023-06-01 11:46:58.679777> Import completed for koantek_triyam_test.qa100_ce_event_order_link\n2023-06-01 11:47:05.816949> Starting to import qa100_CE_EVENT_PRSNL\n2023-06-01 11:48:10.085594> Import completed for koantek_triyam_test.qa100_CE_EVENT_PRSNL\n2023-06-01 11:48:19.635935> Starting to import qa100_clinical_event\n2023-06-01 11:50:18.136605> Import completed for koantek_triyam_test.qa100_clinical_event\n2023-06-01 11:50:21.990639> Starting to import QA100_ORDERS\n2023-06-01 11:50:40.495189> Import completed for koantek_triyam_test.QA100_ORDERS\n2023-06-01 11:50:43.381074> Starting to import Register_fovea_5252022_final\n2023-06-01 11:50:51.368020> Import completed for koantek_triyam_test.Register_fovea_5252022_final\n2023-06-01 11:50:56.642656> Starting to import Tri_Encounter\n2023-06-01 11:54:04.688606> Import completed for koantek_triyam_test.Tri_Encounter\n2023-06-01 11:54:36.155833> Starting to import TRI_Person\n2023-06-01 11:56:50.856290> Import completed for koantek_triyam_test.TRI_Person\n2023-06-01 11:57:04.595020> Starting to import Tri_PRSNL\n2023-06-01 11:57:18.457647> Import completed for koantek_triyam_test.Tri_PRSNL\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1685558345730,"submitTime":1685558345702,"finishTime":1685620644232,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",28246]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"3bb6a750-c4f2-416f-ac48-a16be2ea9853"},{"version":"CommandV1","origId":1343992488731481,"guid":"f23fb265-950e-4c43-9435-ff2eaa54982d","subtype":"command","commandType":"auto","position":1.015625,"command":"koantekParallelLoad(newcfg)","commandVersion":7,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"2023-05-31 05:02:54.244831> Starting to import ACCESSION_ORDER_R\n2023-05-31 05:05:07.148017> Import completed for koantek_triyam.ACCESSION_ORDER_R\n2023-05-31 05:05:28.658083> Starting to import ACT_PW_COMP_1\n2023-05-31 05:13:19.772235> Import completed for koantek_triyam.ACT_PW_COMP_1\n2023-05-31 05:13:43.997998> Starting to import ADDRESS\n2023-05-31 05:17:10.546711> Import completed for koantek_triyam.ADDRESS\n2023-05-31 05:17:31.565422> Starting to import CE_CALCULATION_RESULT\n2023-05-31 05:18:50.377617> Import completed for koantek_triyam.CE_CALCULATION_RESULT\n2023-05-31 05:19:13.790863> Starting to import CE_CONTRIBUTOR_LINK\n2023-05-31 05:20:51.258380> Import completed for koantek_triyam.CE_CONTRIBUTOR_LINK\n2023-05-31 05:21:10.179401> Starting to import CE_DATE_RESULT\n2023-05-31 05:22:57.528388> Import completed for koantek_triyam.CE_DATE_RESULT\n2023-05-31 05:23:23.907797> Starting to import CE_EVENT_NOTE\n2023-05-31 05:26:27.851404> Import completed for koantek_triyam.CE_EVENT_NOTE\n2023-05-31 05:32:17.550764> Starting to import ce_event_order_link\n2023-05-31 05:46:49.780102> Exception attempting to sync for koantek_triyam.ce_event_order_link - retrying in 15 seconds. Exception: An error occurred while calling o898.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 201.0 failed 4 times, most recent failure: Lost task 4.3 in stage 201.0 (TID 532) (10.139.64.4 executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Command exited with code 50\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3481)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3413)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1449)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1449)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3701)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3639)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3627)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1184)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2753)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2736)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$1(FileFormatWriter.scala:299)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:330)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:251)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:115)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:71)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$3(commands.scala:132)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:130)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:129)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$doExecute$4(commands.scala:156)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:156)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:273)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:107)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:330)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:326)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:269)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:358)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:357)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$11(TransactionalWriteEdge.scala:546)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:541)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:654)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:675)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:459)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:444)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:649)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:568)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:529)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:279)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1981)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:278)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:311)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:305)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:592)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:582)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:226)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:223)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:143)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:375)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:172)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:196)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:183)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:160)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:159)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:654)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:675)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:459)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:444)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:649)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:568)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:529)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:23)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:63)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:145)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:104)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:158)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:148)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:138)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:123)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:264)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:93)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:126)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:913)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:265)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:263)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:93)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:872)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:673)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1751)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:654)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:683)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:649)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:229)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:288)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:238)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:220)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:233)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:226)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:226)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:226)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:180)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:287)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:725)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n2023-05-31 06:26:06.630220> Import completed for koantek_triyam.ce_event_order_link\n2023-05-31 06:45:42.977948> Starting to import CE_EVENT_PRSNL\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1685509359327,"submitTime":1685509359327,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",22917]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"1e299540-e0e6-4525-b08e-3207fe29084e"},{"version":"CommandV1","origId":1343992488731482,"guid":"f15b276e-bd9f-4eec-b2aa-3250783ce622","subtype":"command","commandType":"auto","position":3.015625,"command":"udf_gap = {\n        \"config\":\n        {\n            # ALL FIELDS ARE MANDATORY IN THIS DICTIONARY\n             \"jdbcHostname\":\"10.10.9.4\" # Source Server Address\n            ,\"jdbcPort\":1433    # Source Server Port\n            ,\"jdbcSourceDatabase\":\"NCH_Triyam\" # Source Server Database \n            ,\"jdbcUsername\":\"koantekUser\" # Source Server user\n            ,\"jdbcPassword\":\"aBjO40oTOy&4l0\" # Source Server password\n            ,\"jdbcDriver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\" # Source Server JDBC Driver\n            ,\"jdbcSourceSchema\":\"triyam\" # Source Database Schema\n            #,\"targetDatabase\":\"koantek_parallel_load\" Target databse for Unity Catalog implementation\n            ,\"targetSchema\":\"koantek_triyam_test\" # Target Server Schema\n            ,\"parallelCores\":16 # Number of vCPUs available in cluster\n        }\n        ,\"load\":\n    [\n{\n    \"table\":\"ENCNTR_ALIAS\"\n    ,\"keyField\":\"ENCNTR_ALIAS_ID\"\n}\n,\n{\n    \"table\":\"PREGNANCY_CHILD\"\n    ,\"keyField\":\"PREGNANCY_CHILD_ID\"\n}\n,\n{\n    \"table\":\"QA100_CE_EVENT_NOTE\"\n    ,\"keyField\":\"CE_EVENT_NOTE_ID\"\n}\n,\n{\n    \"table\":\"qa100_ORDER_ACTION\"\n    ,\"keyField\":\"ORDER_ID\"\n}\n,\n{\n    \"table\":\"qa100_order_detail\"\n    ,\"keyField\":\"ORDER_ID\"\n}\n,\n{\n    \"table\":\"QA100_V500_EVENT_CODE\"\n    ,\"keyField\":\"EVENT_CD\"\n}\n,\n{\n    \"table\":\"Tri_person_alias\"\n    ,\"keyField\":\"PERSON_ALIAS_ID\"\n}\n,\n{\n    \"table\":\"Tri_V500_EVENT_SET_EXPLODE\"\n    ,\"keyField\":\"EVENT_CD\"\n}\n,\n{\n    \"table\":\"V500_EVENT_SET_CODE\"\n    ,\"keyField\":\"EVENT_SET_CD\"\n}\n]\n}","commandVersion":11,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1685940632759,"submitTime":1685940632732,"finishTime":1685940632836,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"551827f5-21c1-466b-b60b-814be7d36764"},{"version":"CommandV1","origId":1343992488731483,"guid":"85643ff5-f1b8-45bd-a2fa-340a8716566c","subtype":"command","commandType":"auto","position":4.015625,"command":"koantekParallelLoad(udf_gap)","commandVersion":2,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"2023-06-05 04:51:00.335861> Starting to import ENCNTR_ALIAS\n2023-06-05 04:53:02.296249> Import completed for koantek_triyam_test.ENCNTR_ALIAS\n2023-06-05 04:53:15.694084> Starting to import PREGNANCY_CHILD\n2023-06-05 04:53:21.102642> Import completed for koantek_triyam_test.PREGNANCY_CHILD\n2023-06-05 04:53:25.141168> Starting to import QA100_CE_EVENT_NOTE\n2023-06-05 04:53:30.231495> Import completed for koantek_triyam_test.QA100_CE_EVENT_NOTE\n2023-06-05 04:53:35.218396> Starting to import qa100_ORDER_ACTION\n2023-06-05 04:53:56.710949> Import completed for koantek_triyam_test.qa100_ORDER_ACTION\n2023-06-05 04:54:04.011584> Starting to import qa100_order_detail\n2023-06-05 04:54:22.525670> Import completed for koantek_triyam_test.qa100_order_detail\n2023-06-05 04:54:39.915684> Starting to import QA100_V500_EVENT_CODE\n2023-06-05 04:54:44.242209> Import completed for koantek_triyam_test.QA100_V500_EVENT_CODE\n2023-06-05 04:54:49.988928> Starting to import Tri_person_alias\n2023-06-05 04:55:36.145711> Import completed for koantek_triyam_test.Tri_person_alias\n2023-06-05 04:55:42.023254> Starting to import Tri_V500_EVENT_SET_EXPLODE\n2023-06-05 04:55:48.077345> Import completed for koantek_triyam_test.Tri_V500_EVENT_SET_EXPLODE\n2023-06-05 04:55:51.047068> Starting to import V500_EVENT_SET_CODE\n2023-06-05 04:55:54.597021> Import completed for koantek_triyam_test.V500_EVENT_SET_CODE\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1685940649554,"submitTime":1685940649528,"finishTime":1685940957220,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",1390]],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"34129fa1-6b01-4efd-b642-28fd0066a5c9"},{"version":"CommandV1","origId":1343992488731484,"guid":"81bb6a2c-29f7-4a85-80bb-c9a6e76cd2a0","subtype":"command","commandType":"auto","position":5.015625,"command":"vwcfg = {\n    \"config\":\n        {\n            # ALL FIELDS ARE MANDATORY IN THIS DICTIONARY\n             \"jdbcHostname\":\"10.10.9.4\" # Source Server Address\n            ,\"jdbcPort\":1433    # Source Server Port\n            ,\"jdbcSourceDatabase\":\"NCH_Triyam\" # Source Server Database \n            ,\"jdbcUsername\":\"koantekUser\" # Source Server user\n            ,\"jdbcPassword\":\"aBjO40oTOy&4l0\" # Source Server password\n            ,\"jdbcDriver\":\"com.microsoft.sqlserver.jdbc.SQLServerDriver\" # Source Server JDBC Driver\n            ,\"jdbcSourceSchema\":\"triyam\" # Source Database Schema\n            #,\"targetDatabase\":\"koantek_parallel_load\" Target databse for Unity Catalog implementation\n            ,\"targetSchema\":\"koantek_triyam_test\" # Target Server Schema\n            ,\"parallelCores\":16 # Number of vCPUs available in cluster\n        }\n        ,\"load\":\n[     \n    {\n        \"table\":\"vw_NCH_CERNER_MAR_ALL_V1\"\n        ,\"keyField\": \"EVENT_ID\"}\n]    \n}","commandVersion":7,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":1686673454172,"submitTime":1686673454124,"finishTime":1686673454257,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"4be02709-9a4e-4a46-8b75-b0756483c25a"}],"dashboards":[],"guid":"bc52d5ca-6eb8-44e7-a301-ab66856e4820","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":4},"reposExportFormat":"SOURCE"}